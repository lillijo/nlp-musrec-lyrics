{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33ae6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Imports\n",
    "import sys \n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import collections\n",
    "\n",
    "# Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument \n",
    "\n",
    "# Vader and Stopwords\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Clustering, DR and tfidf\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9607189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123984\n"
     ]
    }
   ],
   "source": [
    "with open('tracks.json','r') as f:\n",
    "    tracks = json.load(f)\n",
    "print(len(tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c327feb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histograms(documents):\n",
    "    amt_of_words = []\n",
    "    amt_of_unique_words = []\n",
    "    unique_words = []\n",
    "\n",
    "    for track in documents:\n",
    "        words = track[0]\n",
    "        total_words = len(list(set(words)))\n",
    "        amt_of_words.append(len(words))\n",
    "        amt_of_unique_words.append(total_words)\n",
    "        unique_words = set(list(unique_words) + words)\n",
    "    fig, axs = plt.subplots(2,figsize=(4,6))\n",
    "    axs[0].hist(amt_of_words, bins = 500)\n",
    "    axs[1].hist(amt_of_unique_words, bins = 500)\n",
    "    plt.show() \n",
    "\n",
    "    avg_amt_of_words = sum(amt_of_words) / len(amt_of_words)\n",
    "    avg_amt_of_unique_words = sum(amt_of_unique_words) / len(amt_of_unique_words)\n",
    "    print(\"#Songs: \", len(documents))\n",
    "    print(\"#Unique Words: \", len(unique_words))\n",
    "    print(\"Longest Song: \", max(amt_of_words))\n",
    "    print(\"Avg Wordcount: \", int(avg_amt_of_words)  )\n",
    "    print(\"Avg Unique Wordcount: \",int(avg_amt_of_unique_words))\n",
    "\n",
    "#plot_histograms(tracks)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a688c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(documents):\n",
    "    train_documents = [TaggedDocument(i[0],[i[1][0]]) for i in documents]\n",
    "    model = Doc2Vec(documents=train_documents, vector_size=10, \n",
    "                    epochs=10, min_count=2, workers=6, window=5,\n",
    "                    sample=0.000001)\n",
    "    model.save(\"doc2vec.model\")\n",
    "    return model\n",
    "\n",
    "def infer_vectors(documents, model):\n",
    "    inferred_vectors = {}\n",
    "    for doc_id in [i[1][0] for i in documents]:\n",
    "        inferred_vector = model.dv[doc_id]\n",
    "        inferred_vectors[doc_id] = inferred_vector\n",
    "    with open(\"inferred_vectors_dict.p\",\"wb\") as inf_json:\n",
    "        pickle.dump(inferred_vectors,inf_json)\n",
    "    return inferred_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fbfdfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf_df(documents):\n",
    "    corpus = [' '.join(x[0]) for x in documents]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_x = vectorizer.fit_transform(corpus)\n",
    "    df_countvect = pd.DataFrame(data= tfidf_x.todense(), index=[i[1][0] for i in documents], \n",
    "                                columns=vectorizer.get_feature_names() )\n",
    "    with open(\"tfidf_df.p\",\"wb\") as tfidfp:\n",
    "        pickle.dump(df_countvect, tfidfp)  \n",
    "    return df_countvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c88f6be4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def do_clustering(df, inferred):\n",
    "    kmeans_model = KMeans(init='k-means++', n_clusters=20, n_init=10)\n",
    "    kmeans_model.fit(np.array(list(inferred.values())))\n",
    "    clustering = kmeans_model.labels_\n",
    "    clust_df = pd.DataFrame({'cluster': clustering}, index=[i for i in inferred.keys()])\n",
    "    new_df = df.join(clust_df)\n",
    "    with open(\"tfidf_df.p\",\"wb\") as inf_json:\n",
    "        pickle.dump(new_df,inf_json)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853956fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_genre_todf(df):\n",
    "    with open('test_with_genres_dict.json') as json_file:\n",
    "        test_dictionary = json.load(json_file)\n",
    "    genres = [i['index'] for i in test_dictionary.values()]\n",
    "    clust_df = pd.DataFrame({'genre': genres}, index=[i for i in test_dictionary.keys()] )\n",
    "    new_df = df.join(clust_df)\n",
    "    new_df['genre'] = new_df['genre'].replace(np.nan, 7)\n",
    "    new_df['genre'] = pd.to_numeric(new_df['genre'], downcast='signed')\n",
    "    with open(\"tfidf_df.p\",\"wb\") as tfidfp:\n",
    "        pickle.dump(new_df, tfidfp)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f534f26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(documents):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    polarity_scores = {}\n",
    "    for doc in documents:\n",
    "        polarity_scores[doc[1][0]] = sia.polarity_scores(' '.join(set(doc[0])))\n",
    "    with open(\"sia_scores\",\"wb\") as siaf:\n",
    "        pickle.dump(sia_scores,siaf)\n",
    "    return polarity_scores\n",
    "\n",
    "#sia_scores = sentiment_analyzer(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70236843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_with_topwords(X_embedded= None, df_count_vect=None, new_clustering=False, new_tsne=False):\n",
    "    # load data\n",
    "    # vectors from doc2vec model doc_id: [vector]\n",
    "    test_dictionary = pickle.load( open( \"test_dict.p\", \"rb\" ) )\n",
    "    all_inferred = pickle.load( open( \"inferred_vectors_dict.p\", \"rb\" ) )\n",
    "    sia_scores = pickle.load( open( \"sia_scores.p\", \"rb\" ) ) \n",
    "    test_sias = {x:sia_scores[x] for x in test_dictionary.keys()}\n",
    "    # tf-idf matrix: doc_id w1 w2 ... wn\n",
    "    if df_count_vect is None:\n",
    "        df_count_vect = pickle.load( open( \"tfidf_df.p\", \"rb\" ) )\n",
    "    genre_labels = {i['index']:i['name'] for i in test_dictionary.values()}\n",
    "\n",
    "    # np array of sia scores\n",
    "    X_sia = np.array([list(test_sias[i].values()) for i in test_sias.keys()])\n",
    "\n",
    "    # np array of vectors\n",
    "    X = np.array([all_inferred[i] for i in test_dictionary.keys()])\n",
    "    \n",
    "    # apply t-SNE\n",
    "    if new_tsne or X_embedded is None:\n",
    "        X_embedded = TSNE(n_components=2, perplexity=100, learning_rate=200).fit_transform(X)\n",
    "        X_embedded.shape\n",
    "    \n",
    "    tfidf_vecs = [] #df_countvect.loc(0)[list(test_dictionary.keys())]\n",
    "    clustering = [] #list(tfidf_vecs['cluster'].to_numpy())\n",
    "    if new_clustering:\n",
    "        # cluster\n",
    "        df_countvect = do_clustering(df_count_vect, all_inferred)\n",
    "        tfidf_vecs = df_count_vect.loc(0)[list(test_dictionary.keys())]\n",
    "        clustering = list(tfidf_vecs['cluster'].to_numpy())\n",
    "        print(set(clustering))\n",
    "    else:\n",
    "        tfidf_vecs = df_count_vect.loc(0)[list(test_dictionary.keys())]\n",
    "        clustering = list(tfidf_vecs['cluster'].to_numpy())   \n",
    "\n",
    "    cluster_labels = []\n",
    "    for i in range(20):\n",
    "        cluster = df_count_vect.loc(0)[df_count_vect.loc(1)['cluster'] == i]\n",
    "        cluster_mean = cluster.mean()\n",
    "        top_words = cluster_mean.sort_values()\n",
    "        cluster_labels.append(top_words[-30:])\n",
    "        \n",
    "    genre_topwords = []\n",
    "    for i in range(20):\n",
    "        gcluster = df_count_vect.loc(0)[df_count_vect.loc(1)['genre'] == i]\n",
    "        gcluster_mean = gcluster.mean()\n",
    "        gtop_words = gcluster_mean.sort_values()\n",
    "        genre_topwords.append(gtop_words[-30:])\n",
    "    genre_lab = [genre_labels[i] +': '+ ' , '.join(list(genre_topwords[i].keys().to_numpy())) for i in range(20)]\n",
    "        \n",
    "    return X_embedded, clustering, cluster_labels, genre_lab, test_dictionary, genre_topwords, X_sia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2724128",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = ['#e6194b', '#3cb44b', '#ffe119','#ffd8b1', '#aaffc3', '#fffac8', '#808080', \n",
    "          '#4363d8', '#f58231', '#911eb4', \n",
    "          '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#808000', \n",
    "          '#9a6324', '#000075', '#800000',  ]\n",
    "\n",
    "\n",
    "def draw_scatter(data, cs, n_clust=20):\n",
    "    plt.figure(figsize=(25, 25))\n",
    "    plt.scatter([i[0] for i in data],[i[1] for i in data], color=[colors[i] for i in cs], s=30)\n",
    "    \n",
    "def draw_legend(labels, n_clust=20):\n",
    "    plt.figure(figsize=(25, 10))\n",
    "    plt.scatter([-10 for i in range(n_clust)],[i*3 for i in range(n_clust)], color=[colors[i] for i in list(range(0,n_clust))], cmap='tab20', s=200, marker=\"s\")\n",
    "    plt.scatter([0],[0],c=[0], cmap='tab20', s=5, marker=\"s\")\n",
    "    for i in list(range(0,n_clust)):\n",
    "        txt = labels[i] if isinstance(labels[i], str) else ', '.join(list(labels[i].keys().to_numpy()))\n",
    "        plt.annotate(txt, (-9.8,  i*3),fontsize=24, va='center')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a591507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this method to intialize all data for the first time\n",
    "def run_all_initial():\n",
    "    model = train_model(tracks)\n",
    "    inferred_vectors_dict = infer_vectors(tracks, model)\n",
    "\n",
    "    df_countvect = build_tfidf_df(tracks)\n",
    "    #df_countvect = pickle.load( open( \"tfidf_df.p\", \"rb\" ) ) \n",
    "    if not 'genre' in df_countvect:\n",
    "        df_countvect = add_genre_todf(df_countvect)\n",
    "    inferred_vectors_dict = pickle.load( open( \"inferred_vectors_dict.p\", \"rb\" ) ) \n",
    "    if not 'cluster' in df_countvect:\n",
    "        df_countvect = do_clustering(df_countvect, inferred_vectors_dict)\n",
    "    return model, inferred_vectors_dict, df_countvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9de3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this method to recompute 'cluster' info and load model etc.\n",
    "def run_all_secondary():\n",
    "    model = Doc2Vec.load(\"doc2vec.model\")\n",
    "    inferred_vectors_dict = pickle.load( open( \"inferred_vectors_dict.p\", \"rb\" ) ) \n",
    "    df_countvect = pickle.load( open( \"tfidf_df.p\", \"rb\" ) ) \n",
    "    if not 'genre' in df_countvect:\n",
    "        df_countvect = add_genre_todf(df_countvect)\n",
    "    if not 'cluster' in df_countvect:\n",
    "        df_countvect = do_clustering(df_countvect, inferred_vectors_dict)\n",
    "    return model, inferred_vectors_dict, df_countvect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0123b0ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def visualize_genre_clusters():\n",
    "\n",
    "    tsne,clusters,c_labels,genres,dict_test, gen_tw, sia_embed = compute_test_with_topwords(\n",
    "        X_embedded=None, df_count_vect=None,new_clustering=False, new_tsne=False) \n",
    "\n",
    "    draw_scatter(tsne, clusters)\n",
    "    draw_legend(c_labels)\n",
    "    draw_scatter(tsne, [i['index'] for i in dict_test.values()])\n",
    "    draw_legend(genres)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
